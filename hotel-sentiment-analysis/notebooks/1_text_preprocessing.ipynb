{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1_text_preprocessing.ipynb\n",
    "# Purpose: Load Chinese hotel review data, perform preprocessing, tokenization, and word index mapping\n",
    "import os\n",
    "import re\n",
    "import bz2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load positive and negative samples (local, not to be published)\n",
    "train_texts = []\n",
    "train_labels = []\n",
    "\n",
    "with open(\"positive_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        dic = eval(line.strip())\n",
    "        train_texts.append(dic[\"text\"])\n",
    "        train_labels.append(dic[\"label\"])\n",
    "\n",
    "with open(\"negative_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        dic = eval(line.strip())\n",
    "        train_texts.append(dic[\"text\"])\n",
    "        train_labels.append(dic[\"label\"])\n",
    "\n",
    "# Display number of samples\n",
    "print(\"Total samples:\", len(train_texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained word vectors (Zhihu bigram)\n",
    "if not os.path.exists(\"embeddings/sgns.zhihu.bigram\"):\n",
    "    with open(\"embeddings/sgns.zhihu.bigram\", 'wb') as new_file, open(\"embeddings/sgns.zhihu.bigram.bz2\", 'rb') as file:\n",
    "        decompressor = bz2.BZ2Decompressor()\n",
    "        for data in iter(lambda: file.read(100 * 1024), b''):\n",
    "            new_file.write(decompressor.decompress(data))\n",
    "\n",
    "# Load the word vector model\n",
    "cn_model = KeyedVectors.load_word2vec_format(\"embeddings/sgns.zhihu.bigram\", binary=False, unicode_errors=\"ignore\")\n",
    "embedding_dim = cn_model.vector_size\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each sentence into a list of word indexes\n",
    "train_tokens = []\n",
    "for text in train_texts:\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r\"[\\s+\\.!/_,$%^*(+\\\"']+|[+\\-\\-！，。？、~@#￥%……&*（）]+\", \"\", text)\n",
    "    words = list(jieba.cut(text))\n",
    "    word_indexes = []\n",
    "    for word in words:\n",
    "        index = cn_model.key_to_index.get(word, 0)\n",
    "        if index >= 50000:\n",
    "            index = 0\n",
    "        word_indexes.append(index)\n",
    "    train_tokens.append(word_indexes)\n",
    "\n",
    "# Analyze token lengths\n",
    "num_tokens = np.array([len(t) for t in train_tokens])\n",
    "print(\"Average token length:\", np.mean(num_tokens))\n",
    "print(\"Max token length:\", np.max(num_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set max token length to cover ~95% of samples\n",
    "max_tokens = int(np.mean(num_tokens) + 2 * np.std(num_tokens))\n",
    "print(\"Max tokens to pad:\", max_tokens)\n",
    "\n",
    "# Padding and truncating\n",
    "train_pad = pad_sequences(train_tokens, maxlen=max_tokens, padding='pre', truncating='pre')\n",
    "train_pad[train_pad >= 50000] = 0\n",
    "train_labels = np.array(train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token length histogram\n",
    "plt.hist(np.log(num_tokens), bins=10)\n",
    "plt.xlabel('Log(Token Length)')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.title('Token Length Distribution')\n",
    "plt.show()\n",
    "\n",
    "# Reverse function for debugging\n",
    "index_to_word = {v: k for k, v in cn_model.key_to_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_tokens(tokens):\n",
    "    return ''.join(index_to_word.get(i, ' ') for i in tokens if i != 0)\n",
    "\n",
    "# Print example\n",
    "print(\"Original text:\", train_texts[0])\n",
    "print(\"Tokenized (reconstructed):\", reverse_tokens(train_pad[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
