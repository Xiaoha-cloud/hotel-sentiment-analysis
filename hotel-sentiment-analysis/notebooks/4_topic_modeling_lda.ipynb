{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4_topic_modeling_lda.ipynb\n",
    "# Purpose: Perform LDA topic modeling on Chinese hotel reviews and visualize results\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original review texts (assumes pre-tokenized data)\n",
    "with open(\"positive_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    positive_texts = [eval(line.strip())[\"text\"] for line in f]\n",
    "\n",
    "with open(\"negative_samples.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    negative_texts = [eval(line.strip())[\"text\"] for line in f]\n",
    "\n",
    "# Combine all reviews\n",
    "all_texts = positive_texts + negative_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing and tokenization\n",
    "stopwords = set()\n",
    "if not stopwords:\n",
    "    stopwords = set(line.strip() for line in open(\"data/stopwords.txt\", encoding=\"utf-8\"))\n",
    "\n",
    "def clean_and_tokenize(text):\n",
    "    text = re.sub(r\"[\\s+\\.!/_,$%^*(+\\\"']+|[+\\-\\-！，。？、~@#￥%……&*（）]+\", \"\", text)\n",
    "    words = [w for w in jieba.lcut(text) if w not in stopwords and len(w) > 1]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenizer\n",
    "tokenized_texts = [clean_and_tokenize(text) for text in all_texts]\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(tokenized_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model\n",
    "num_topics = 5\n",
    "lda_model = models.LdaModel(corpus=corpus,\n",
    "                             id2word=dictionary,\n",
    "                             num_topics=num_topics,\n",
    "                             random_state=42,\n",
    "                             passes=10,\n",
    "                             alpha='auto',\n",
    "                             per_word_topics=True)\n",
    "\n",
    "# Display top keywords per topic\n",
    "for i, topic in lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {i}: {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "lda_vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(lda_vis)\n",
    "\n",
    "# Save HTML visualization\n",
    "pyLDAvis.save_html(lda_vis, \"results/lda_visualization.html\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
