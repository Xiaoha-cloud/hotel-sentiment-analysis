{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3_sentiment_lstm_model.ipynb\n",
    "# Purpose: Train a BiLSTM model for sentiment classification on Chinese hotel reviews\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load processed data from previous notebook\n",
    "# Assumes train_pad and train_labels are available from preprocessing\n",
    "train_pad = np.load(\"data/train_pad.npy\")\n",
    "train_labels = np.load(\"data/train_labels.npy\")\n",
    "embedding_matrix = np.load(\"data/embedding_matrix.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "max_tokens = train_pad.shape[1]       # Sequence length\n",
    "embedding_dim = embedding_matrix.shape[1]\n",
    "num_words = embedding_matrix.shape[0] # Vocabulary size\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_pad, train_labels, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the BiLSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_dim,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=max_tokens,\n",
    "                    trainable=False))\n",
    "model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
    "model.add(LSTM(units=16, return_sequences=False))\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up callbacks\n",
    "log_dir = \"logs/sentiment_lstm\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(log_dir, \"sentiment_checkpoint.keras\")\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
    "    ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=1),\n",
    "    TensorBoard(log_dir=log_dir)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.1,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy: {:.2f}%\".format(results[1] * 100))\n",
    "\n",
    "# Save the final model\n",
    "model.save(os.path.join(log_dir, \"final_sentiment_model.h5\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
